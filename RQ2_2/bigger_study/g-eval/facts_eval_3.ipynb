{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_json, load_txt\n",
    "first_paragraph = load_json(\"../bigger_study_sample/001-57899.json\")['facts']\n",
    "second_paragraph = load_txt(\"./gpt-4/001-57899.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ahmed/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/ahmed/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    processed_sentences = []\n",
    "    for sentence in sentences:\n",
    "        # Tokenize into words\n",
    "        words = word_tokenize(sentence)\n",
    "        # Lowercase and remove punctuation\n",
    "        words = [word.lower() for word in words if word.isalnum()]\n",
    "        # Remove stop words\n",
    "        words = [word for word in words if word not in stopwords.words('english')]\n",
    "        processed_sentences.append(' '.join(words))\n",
    "    return processed_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def extract_facts(sentences):\n",
    "    facts = []\n",
    "    for sentence in sentences:\n",
    "        doc = nlp(sentence)\n",
    "        for token in doc:\n",
    "            # Look for subject-verb-object constructs\n",
    "            if token.dep_ == 'ROOT':\n",
    "                subject = [w.text for w in token.lefts if w.dep_ in ('nsubj', 'nsubjpass')]\n",
    "                object_ = [w.text for w in token.rights if w.dep_ in ('dobj', 'pobj')]\n",
    "                if subject and object_:\n",
    "                    facts.append({\n",
    "                        'subject': subject[0],\n",
    "                        'verb': token.text,\n",
    "                        'object': object_[0]\n",
    "                    })\n",
    "    return facts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ahmed/thesis24/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def facts_to_sentences(facts):\n",
    "    return ['{} {} {}'.format(fact['subject'], fact['verb'], fact['object']) for fact in facts]\n",
    "\n",
    "def compare_facts(facts1, facts2):\n",
    "    sentences1 = facts_to_sentences(facts1)\n",
    "    sentences2 = facts_to_sentences(facts2)\n",
    "    \n",
    "    vectorizer = TfidfVectorizer().fit(sentences1 + sentences2)\n",
    "    vectors1 = vectorizer.transform(sentences1)\n",
    "    vectors2 = vectorizer.transform(sentences2)\n",
    "    \n",
    "    discrepancies = []\n",
    "    for i, vec2 in enumerate(vectors2):\n",
    "        similarities = cosine_similarity(vec2, vectors1)\n",
    "        max_similarity = similarities.max()\n",
    "        if max_similarity < 0.5:  # Threshold for similarity\n",
    "            discrepancies.append(facts2[i])\n",
    "    return discrepancies\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def compare_facts_advanced(facts1, facts2):\n",
    "    sentences1 = facts_to_sentences(facts1)\n",
    "    sentences2 = facts_to_sentences(facts2)\n",
    "    \n",
    "    embeddings1 = model.encode(sentences1)\n",
    "    embeddings2 = model.encode(sentences2)\n",
    "    \n",
    "    discrepancies = []\n",
    "    for i, emb2 in enumerate(embeddings2):\n",
    "        similarities = cosine_similarity([emb2], embeddings1)\n",
    "        max_similarity = similarities.max()\n",
    "        if max_similarity < 0.5:\n",
    "            discrepancies.append(facts2[i])\n",
    "    return discrepancies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Facts in the second text not mentioned in the first:\n",
      "- case involves boner\n",
      "- case follows 1\n",
      "- judge ruled presence\n",
      "- evidence implicated robbery\n",
      "- parties involved boner\n",
      "- echr ruled article\n",
      "- justice required hearing\n",
      "- boner represented appeal\n",
      "- justice required representation\n"
     ]
    }
   ],
   "source": [
    "def report_discrepancies(discrepancies):\n",
    "    print(\"Facts in the second text not mentioned in the first:\")\n",
    "    for fact in discrepancies:\n",
    "        print('- {} {} {}'.format(fact['subject'], fact['verb'], fact['object']))\n",
    "\n",
    "text1, text2 = first_paragraph, second_paragraph\n",
    "\n",
    "# Example usage:\n",
    "text1_processed = preprocess_text(text1)\n",
    "text2_processed = preprocess_text(text2)\n",
    "\n",
    "facts1 = extract_facts(text1_processed)\n",
    "facts2 = extract_facts(text2_processed)\n",
    "\n",
    "discrepancies = compare_facts(facts1, facts2)\n",
    "\n",
    "report_discrepancies(discrepancies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Facts in the second text not mentioned in the first:\n",
      "- case follows 1\n",
      "- evidence implicated robbery\n",
      "- echr ruled article\n"
     ]
    }
   ],
   "source": [
    "def report_discrepancies(discrepancies):\n",
    "    print(\"Facts in the second text not mentioned in the first:\")\n",
    "    for fact in discrepancies:\n",
    "        print('- {} {} {}'.format(fact['subject'], fact['verb'], fact['object']))\n",
    "\n",
    "text1, text2 = first_paragraph, second_paragraph\n",
    "\n",
    "# Example usage:\n",
    "text1_processed = preprocess_text(text1)\n",
    "text2_processed = preprocess_text(text2)\n",
    "\n",
    "facts1 = extract_facts(text1_processed)\n",
    "facts2 = extract_facts(text2_processed)\n",
    "\n",
    "discrepancies = compare_facts_advanced(facts1, facts2)\n",
    "\n",
    "report_discrepancies(discrepancies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def build_knowledge_graph(facts):\n",
    "    G = nx.Graph()\n",
    "    for fact in facts:\n",
    "        subject = fact['subject']\n",
    "        verb = fact['verb']\n",
    "        object_ = fact['object']\n",
    "        G.add_edge(subject, object_, label=verb)\n",
    "    return G\n",
    "\n",
    "# Build graphs for both texts\n",
    "graph1 = build_knowledge_graph(facts1)\n",
    "graph2 = build_knowledge_graph(facts2)\n",
    "\n",
    "# Compare the graphs to find discrepancies\n",
    "def compare_graphs(graph1, graph2):\n",
    "    discrepancies = []\n",
    "    edges1 = set(graph1.edges(data='label'))\n",
    "    edges2 = set(graph2.edges(data='label'))\n",
    "    new_edges = edges2 - edges1\n",
    "    for edge in new_edges:\n",
    "        discrepancies.append(edge)\n",
    "    return discrepancies\n",
    "\n",
    "discrepancies = compare_graphs(graph1, graph2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('events', 'employees', 'assaulted'),\n",
       " ('boner', 'parties', 'involved'),\n",
       " ('judge', 'presence', 'ruled'),\n",
       " ('case', 'boner', 'involves'),\n",
       " ('boner', 'appeal', 'represented'),\n",
       " ('justice', 'hearing', 'required'),\n",
       " ('evidence', 'robbery', 'implicated'),\n",
       " ('case', '1', 'follows'),\n",
       " ('echr', 'article', 'ruled'),\n",
       " ('justice', 'representation', 'required')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discrepancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EdgeDataView([('course', 'employees', 'assaulted'), ('applicant', 'trial', 'received'), ('applicant', 'fact', 'included'), ('applicant', 'application', 'granted'), ('trial', 'appeal', 'obtained'), ('mrs', 'evidence', 'give'), ('mrs', 'office', 'stated'), ('inquiries', 'responsibility', 'revealed'), ('judge', 'injustice', 'took'), ('judge', 'imprisonment', 'sentenced'), ('witness', 'boner', 'identified'), ('boner', 'solicitors', 'instructed'), ('boner', 'commission', 'applied'), ('jury', 'charges', 'found'), ('solicitor', 'conviction', 'lodged'), ('solicitors', 'appeal', 'filed'), ('appeal', 'grounds', 'contained'), ('appeal', 'board', 'see'), ('appeal', 'bench', 'heard'), ('grounds', 'court', 'appeal'), ('work', 'aid', 'covered'), ('aid', 'recommendation', 'made'), ('application', 'proceedings', 'extend'), ('application', 'reconsideration', 'involves'), ('application', 'commission', 'declared'), ('opinion', 'board', 'forwarded'), ('opinion', 'judgment', 'reproduced'), ('board', 'decisions', 'takes'), ('counsel', 'question', 'reiterated'), ('submissions', 'case', 'presented'), ('court', 'effect', 'issued'), ('person', 'charge', 'convicted'), ('appellant', 'review', 'ask'), ('appellant', 'act', 'appeal'), ('review', 'decisions', 'subject'), ('act', 'decision', 'seek'), ('advocate', 'duties', 'imposes'), ('31', 'votes', 'concluded')])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph1.edges(data='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f36635f193cf441f84a8861148097e20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31795ec33aa04512b02d6e71a747fd2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9061cda1552244a18ef57d16f603e225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/3.73k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be132528cdc944188fc1cf9686554f29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7be4bf9bd484354b6f1e38077121182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bca47fe07e8940999dabb16b9afe014e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69b72729e8f64862bfb18e40b8bb42a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ade9814bff0342f9b8050c950d235c89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6317700a43884d5eabbd013804147ea4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cce799969a4c471fb67193b70c5c24bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cc2949a250948fbbcf247f5568a1b7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Facts in Paragraph 2: []\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load spaCy model for NER, POS tagging, dependency parsing\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Load a sentence transformer model for semantic similarity\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "def preprocess(text):\n",
    "    # Tokenize and extract entities, facts from a paragraph\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    return sentences, entities\n",
    "\n",
    "def extract_facts(sentences):\n",
    "    # Using dependency parsing to extract simple facts\n",
    "    facts = []\n",
    "    for sent in sentences:\n",
    "        doc = nlp(sent)\n",
    "        # Example fact extraction logic using subject-predicate-object relations\n",
    "        for token in doc:\n",
    "            if token.dep_ == 'ROOT':  # The main verb of the sentence\n",
    "                subj = [w for w in token.lefts if w.dep_ == 'nsubj']\n",
    "                obj = [w for w in token.rights if w.dep_ in ('dobj', 'pobj')]\n",
    "                if subj and obj:\n",
    "                    facts.append((subj[0], token, obj[0]))  # (subject, verb, object)\n",
    "    return facts\n",
    "\n",
    "def compare_facts(facts1, facts2):\n",
    "    # Convert facts to text and then to embeddings\n",
    "    facts1_text = [' '.join([str(f) for f in fact]) for fact in facts1]\n",
    "    facts2_text = [' '.join([str(f) for f in fact]) for fact in facts2]\n",
    "    \n",
    "    # Calculate semantic similarity\n",
    "    embeddings1 = model.encode(facts1_text, convert_to_tensor=True)\n",
    "    embeddings2 = model.encode(facts2_text, convert_to_tensor=True)\n",
    "    \n",
    "    # Find new facts by comparing similarity\n",
    "    new_facts = []\n",
    "    for idx, fact2 in enumerate(facts2_text):\n",
    "        similarity = util.pytorch_cos_sim(embeddings2[idx], embeddings1)\n",
    "        if similarity.max() < 0.8:  # Threshold for considering a fact as new\n",
    "            new_facts.append(fact2)\n",
    "    return new_facts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Facts in Paragraph 2: ['case involves Boner', 'Events committed robbery', 'evidence implicated Boner', 'Boner represented himself', 'interests required representation']\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "paragraph1 = first_paragraph\n",
    "paragraph2 = second_paragraph\n",
    "\n",
    "sentences1, entities1 = preprocess(paragraph1)\n",
    "sentences2, entities2 = preprocess(paragraph2)\n",
    "\n",
    "facts1 = extract_facts(sentences1)\n",
    "facts2 = extract_facts(sentences2)\n",
    "\n",
    "new_facts = compare_facts(facts1, facts2)\n",
    "print(\"New Facts in Paragraph 2:\", new_facts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name nlpaueb/legal-bert-base-uncased. Creating a new one with mean pooling.\n",
      "/Users/ahmed/thesis24/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import spacy\n",
    "\n",
    "# Load spaCy model and legal-specific sentence transformer model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "legal_model = SentenceTransformer('nlpaueb/legal-bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Add custom rules to handle legal abbreviations (e.g., \"Art.\" for Article, \"Sec.\" for Section)\n",
    "@Language.component(\"legal_abbreviation_rules\")\n",
    "def legal_abbreviation_rules(doc):\n",
    "    # Define a set of common legal abbreviations\n",
    "    legal_abbreviations = {\"Art.\", \"Sec.\", \"Mr.\", \"Mrs.\", \"Dr.\", \"Ms.\", \"Prof.\", \"Gov.\"}\n",
    "    \n",
    "    for token in doc[:-1]:  # Iterate over the tokens but skip the last token\n",
    "        if token.text in legal_abbreviations and token.i < len(doc) - 1:\n",
    "            next_token = doc[token.i + 1]\n",
    "            if next_token.is_title:  # Check if the next token starts a new title\n",
    "                # Merge abbreviation with the next token\n",
    "                with doc.retokenize() as retokenizer:\n",
    "                    span = doc[token.i: token.i + 2]  # Merge abbreviation with next token\n",
    "                    retokenizer.merge(span)\n",
    "    return doc\n",
    "\n",
    "# Register the component with spaCy\n",
    "nlp.add_pipe(\"legal_abbreviation_rules\", before=\"parser\")\n",
    "\n",
    "def extract_sentences(text):\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Extract sentences, ensuring custom handling for legal abbreviations\n",
    "    sentences = [sent.text.strip() for sent in doc.sents]\n",
    "    \n",
    "    # Filter out any overly short sentences that may be due to tokenization noise\n",
    "    sentences = [sent for sent in sentences if len(sent) > 5]\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "def compare_sentences(paragraph1, paragraph2, threshold=0.8):\n",
    "    # Extract sentences\n",
    "    sentences1 = extract_sentences(paragraph1)\n",
    "    sentences2 = extract_sentences(paragraph2)\n",
    "    \n",
    "    # Encode sentences using a legal-specific model\n",
    "    embeddings1 = legal_model.encode(sentences1, convert_to_tensor=True)\n",
    "    embeddings2 = legal_model.encode(sentences2, convert_to_tensor=True)\n",
    "    \n",
    "    # Compare each sentence in paragraph 2 with sentences from paragraph 1\n",
    "    new_sentences = []\n",
    "    for idx, emb2 in enumerate(embeddings2):\n",
    "        cosine_scores = util.pytorch_cos_sim(emb2, embeddings1)\n",
    "        max_score = cosine_scores.max().item()\n",
    "        \n",
    "        # Only consider sentences that are sufficiently dissimilar\n",
    "        if max_score < threshold:\n",
    "            new_sentences.append(sentences2[idx])\n",
    "    \n",
    "    return new_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "[E040] Attempt to access token at 555, max length 555.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/ahmed/Desktop/msc-24/TND/RQ2_2/bigger_study/facts_eval_3.ipynb Cell 22\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ahmed/Desktop/msc-24/TND/RQ2_2/bigger_study/facts_eval_3.ipynb#X60sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m paragraph1 \u001b[39m=\u001b[39m first_paragraph\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ahmed/Desktop/msc-24/TND/RQ2_2/bigger_study/facts_eval_3.ipynb#X60sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m paragraph2 \u001b[39m=\u001b[39m second_paragraph\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ahmed/Desktop/msc-24/TND/RQ2_2/bigger_study/facts_eval_3.ipynb#X60sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m new_facts \u001b[39m=\u001b[39m compare_sentences(paragraph1, paragraph2)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ahmed/Desktop/msc-24/TND/RQ2_2/bigger_study/facts_eval_3.ipynb#X60sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mNew facts in paragraph 2:\u001b[39m\u001b[39m\"\u001b[39m, new_facts)\n",
      "\u001b[1;32m/Users/ahmed/Desktop/msc-24/TND/RQ2_2/bigger_study/facts_eval_3.ipynb Cell 22\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmed/Desktop/msc-24/TND/RQ2_2/bigger_study/facts_eval_3.ipynb#X60sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompare_sentences\u001b[39m(paragraph1, paragraph2, threshold\u001b[39m=\u001b[39m\u001b[39m0.8\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmed/Desktop/msc-24/TND/RQ2_2/bigger_study/facts_eval_3.ipynb#X60sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     \u001b[39m# Extract sentences\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmed/Desktop/msc-24/TND/RQ2_2/bigger_study/facts_eval_3.ipynb#X60sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     sentences1 \u001b[39m=\u001b[39m extract_sentences(paragraph1)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ahmed/Desktop/msc-24/TND/RQ2_2/bigger_study/facts_eval_3.ipynb#X60sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     sentences2 \u001b[39m=\u001b[39m extract_sentences(paragraph2)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmed/Desktop/msc-24/TND/RQ2_2/bigger_study/facts_eval_3.ipynb#X60sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     \u001b[39m# Encode sentences using a legal-specific model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmed/Desktop/msc-24/TND/RQ2_2/bigger_study/facts_eval_3.ipynb#X60sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     embeddings1 \u001b[39m=\u001b[39m legal_model\u001b[39m.\u001b[39mencode(sentences1, convert_to_tensor\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;32m/Users/ahmed/Desktop/msc-24/TND/RQ2_2/bigger_study/facts_eval_3.ipynb Cell 22\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmed/Desktop/msc-24/TND/RQ2_2/bigger_study/facts_eval_3.ipynb#X60sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mextract_sentences\u001b[39m(text):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmed/Desktop/msc-24/TND/RQ2_2/bigger_study/facts_eval_3.ipynb#X60sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     \u001b[39m# Process the text with spaCy\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ahmed/Desktop/msc-24/TND/RQ2_2/bigger_study/facts_eval_3.ipynb#X60sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     doc \u001b[39m=\u001b[39m nlp(text)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmed/Desktop/msc-24/TND/RQ2_2/bigger_study/facts_eval_3.ipynb#X60sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     \u001b[39m# Extract sentences, ensuring custom handling for legal abbreviations\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmed/Desktop/msc-24/TND/RQ2_2/bigger_study/facts_eval_3.ipynb#X60sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     sentences \u001b[39m=\u001b[39m [sent\u001b[39m.\u001b[39mtext\u001b[39m.\u001b[39mstrip() \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m doc\u001b[39m.\u001b[39msents]\n",
      "File \u001b[0;32m~/thesis24/lib/python3.11/site-packages/spacy/language.py:1057\u001b[0m, in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mselect_pipes\u001b[39m(\n\u001b[1;32m   1043\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1044\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[1;32m   1045\u001b[0m     disable: Optional[Union[\u001b[39mstr\u001b[39m, Iterable[\u001b[39mstr\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1046\u001b[0m     enable: Optional[Union[\u001b[39mstr\u001b[39m, Iterable[\u001b[39mstr\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1047\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mDisabledPipes\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1048\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Disable one or more pipeline components. If used as a context\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39m    manager, the pipeline will be restored to the initial state at the end\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m \u001b[39m    of the block. Otherwise, a DisabledPipes object is returned, that has\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m \u001b[39m    a `.restore()` method you can use to undo your changes.\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m \n\u001b[1;32m   1053\u001b[0m \u001b[39m    disable (str or iterable): The name(s) of the pipes to disable\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[39m    enable (str or iterable): The name(s) of the pipes to enable - all others will be disabled\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \n\u001b[1;32m   1056\u001b[0m \u001b[39m    DOCS: https://spacy.io/api/language#select_pipes\u001b[39;00m\n\u001b[0;32m-> 1057\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m     \u001b[39mif\u001b[39;00m enable \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m disable \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1059\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE991)\n",
      "File \u001b[0;32m~/thesis24/lib/python3.11/site-packages/spacy/util.py:1722\u001b[0m, in \u001b[0;36mraise_error\u001b[0;34m(proc_name, proc, docs, e)\u001b[0m\n\u001b[1;32m   1718\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m contextvars_eq_thread_ops():\n\u001b[1;32m   1719\u001b[0m                 warnings\u001b[39m.\u001b[39mwarn(Warnings\u001b[39m.\u001b[39mW111)\n\u001b[0;32m-> 1722\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_lexeme_norms\u001b[39m(vocab, component_name):\n\u001b[1;32m   1723\u001b[0m     lexeme_norms \u001b[39m=\u001b[39m vocab\u001b[39m.\u001b[39mlookups\u001b[39m.\u001b[39mget_table(\u001b[39m\"\u001b[39m\u001b[39mlexeme_norm\u001b[39m\u001b[39m\"\u001b[39m, {})\n\u001b[1;32m   1724\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(lexeme_norms) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m vocab\u001b[39m.\u001b[39mlang \u001b[39min\u001b[39;00m LEXEME_NORM_LANGS:\n",
      "File \u001b[0;32m~/thesis24/lib/python3.11/site-packages/spacy/language.py:1052\u001b[0m, in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mselect_pipes\u001b[39m(\n\u001b[1;32m   1043\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1044\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[1;32m   1045\u001b[0m     disable: Optional[Union[\u001b[39mstr\u001b[39m, Iterable[\u001b[39mstr\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1046\u001b[0m     enable: Optional[Union[\u001b[39mstr\u001b[39m, Iterable[\u001b[39mstr\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1047\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mDisabledPipes\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1048\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Disable one or more pipeline components. If used as a context\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39m    manager, the pipeline will be restored to the initial state at the end\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m \u001b[39m    of the block. Otherwise, a DisabledPipes object is returned, that has\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m \u001b[39m    a `.restore()` method you can use to undo your changes.\u001b[39;00m\n\u001b[0;32m-> 1052\u001b[0m \n\u001b[1;32m   1053\u001b[0m \u001b[39m    disable (str or iterable): The name(s) of the pipes to disable\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[39m    enable (str or iterable): The name(s) of the pipes to enable - all others will be disabled\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \n\u001b[1;32m   1056\u001b[0m \u001b[39m    DOCS: https://spacy.io/api/language#select_pipes\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m     \u001b[39mif\u001b[39;00m enable \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m disable \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1059\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE991)\n",
      "\u001b[1;32m/Users/ahmed/Desktop/msc-24/TND/RQ2_2/bigger_study/facts_eval_3.ipynb Cell 22\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ahmed/Desktop/msc-24/TND/RQ2_2/bigger_study/facts_eval_3.ipynb#X60sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m@Language\u001b[39m\u001b[39m.\u001b[39mcomponent(\u001b[39m\"\u001b[39m\u001b[39mlegal_abbreviation_rules\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ahmed/Desktop/msc-24/TND/RQ2_2/bigger_study/facts_eval_3.ipynb#X60sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlegal_abbreviation_rules\u001b[39m(doc):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmed/Desktop/msc-24/TND/RQ2_2/bigger_study/facts_eval_3.ipynb#X60sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m# Define a set of common legal abbreviations\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmed/Desktop/msc-24/TND/RQ2_2/bigger_study/facts_eval_3.ipynb#X60sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     legal_abbreviations \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mArt.\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mSec.\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mMr.\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mMrs.\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDr.\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mMs.\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mProf.\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mGov.\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ahmed/Desktop/msc-24/TND/RQ2_2/bigger_study/facts_eval_3.ipynb#X60sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mfor\u001b[39;49;00m token \u001b[39min\u001b[39;49;00m doc[:\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]:  \u001b[39m# Iterate over the tokens but skip the last token\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmed/Desktop/msc-24/TND/RQ2_2/bigger_study/facts_eval_3.ipynb#X60sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         \u001b[39mif\u001b[39;49;00m token\u001b[39m.\u001b[39;49mtext \u001b[39min\u001b[39;49;00m legal_abbreviations \u001b[39mand\u001b[39;49;00m token\u001b[39m.\u001b[39;49mi \u001b[39m<\u001b[39;49m \u001b[39mlen\u001b[39;49m(doc) \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmed/Desktop/msc-24/TND/RQ2_2/bigger_study/facts_eval_3.ipynb#X60sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m             next_token \u001b[39m=\u001b[39;49m doc[token\u001b[39m.\u001b[39;49mi \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m]\n",
      "File \u001b[0;32m~/thesis24/lib/python3.11/site-packages/spacy/tokens/span.pyx:207\u001b[0m, in \u001b[0;36m__iter__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/thesis24/lib/python3.11/site-packages/spacy/tokens/doc.pyx:505\u001b[0m, in \u001b[0;36mspacy.tokens.doc.Doc.__getitem__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/thesis24/lib/python3.11/site-packages/spacy/tokens/token.pxd:25\u001b[0m, in \u001b[0;36mspacy.tokens.token.Token.cinit\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: [E040] Attempt to access token at 555, max length 555."
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "paragraph1 = first_paragraph\n",
    "paragraph2 = second_paragraph\n",
    "\n",
    "new_facts = compare_sentences(paragraph1, paragraph2)\n",
    "print(\"New facts in paragraph 2:\", new_facts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "[E040] Attempt to access token at 555, max length 555.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/ahmed/Desktop/msc-24/TND/RQ2_2/bigger_study/facts_eval_3.ipynb Cell 23\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ahmed/Desktop/msc-24/TND/RQ2_2/bigger_study/facts_eval_3.ipynb#X61sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m sentences1 \u001b[39m=\u001b[39m extract_sentences(second_paragraph)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ahmed/Desktop/msc-24/TND/RQ2_2/bigger_study/facts_eval_3.ipynb#X61sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m sentences1\n",
      "\u001b[1;32m/Users/ahmed/Desktop/msc-24/TND/RQ2_2/bigger_study/facts_eval_3.ipynb Cell 23\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmed/Desktop/msc-24/TND/RQ2_2/bigger_study/facts_eval_3.ipynb#X61sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mextract_sentences\u001b[39m(text):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmed/Desktop/msc-24/TND/RQ2_2/bigger_study/facts_eval_3.ipynb#X61sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     \u001b[39m# Process the text with spaCy\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ahmed/Desktop/msc-24/TND/RQ2_2/bigger_study/facts_eval_3.ipynb#X61sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     doc \u001b[39m=\u001b[39m nlp(text)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmed/Desktop/msc-24/TND/RQ2_2/bigger_study/facts_eval_3.ipynb#X61sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     \u001b[39m# Extract sentences, ensuring custom handling for legal abbreviations\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmed/Desktop/msc-24/TND/RQ2_2/bigger_study/facts_eval_3.ipynb#X61sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     sentences \u001b[39m=\u001b[39m [sent\u001b[39m.\u001b[39mtext\u001b[39m.\u001b[39mstrip() \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m doc\u001b[39m.\u001b[39msents]\n",
      "File \u001b[0;32m~/thesis24/lib/python3.11/site-packages/spacy/language.py:1057\u001b[0m, in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mselect_pipes\u001b[39m(\n\u001b[1;32m   1043\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1044\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[1;32m   1045\u001b[0m     disable: Optional[Union[\u001b[39mstr\u001b[39m, Iterable[\u001b[39mstr\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1046\u001b[0m     enable: Optional[Union[\u001b[39mstr\u001b[39m, Iterable[\u001b[39mstr\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1047\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mDisabledPipes\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1048\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Disable one or more pipeline components. If used as a context\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39m    manager, the pipeline will be restored to the initial state at the end\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m \u001b[39m    of the block. Otherwise, a DisabledPipes object is returned, that has\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m \u001b[39m    a `.restore()` method you can use to undo your changes.\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m \n\u001b[1;32m   1053\u001b[0m \u001b[39m    disable (str or iterable): The name(s) of the pipes to disable\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[39m    enable (str or iterable): The name(s) of the pipes to enable - all others will be disabled\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \n\u001b[1;32m   1056\u001b[0m \u001b[39m    DOCS: https://spacy.io/api/language#select_pipes\u001b[39;00m\n\u001b[0;32m-> 1057\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m     \u001b[39mif\u001b[39;00m enable \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m disable \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1059\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE991)\n",
      "File \u001b[0;32m~/thesis24/lib/python3.11/site-packages/spacy/util.py:1722\u001b[0m, in \u001b[0;36mraise_error\u001b[0;34m(proc_name, proc, docs, e)\u001b[0m\n\u001b[1;32m   1718\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m contextvars_eq_thread_ops():\n\u001b[1;32m   1719\u001b[0m                 warnings\u001b[39m.\u001b[39mwarn(Warnings\u001b[39m.\u001b[39mW111)\n\u001b[0;32m-> 1722\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_lexeme_norms\u001b[39m(vocab, component_name):\n\u001b[1;32m   1723\u001b[0m     lexeme_norms \u001b[39m=\u001b[39m vocab\u001b[39m.\u001b[39mlookups\u001b[39m.\u001b[39mget_table(\u001b[39m\"\u001b[39m\u001b[39mlexeme_norm\u001b[39m\u001b[39m\"\u001b[39m, {})\n\u001b[1;32m   1724\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(lexeme_norms) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m vocab\u001b[39m.\u001b[39mlang \u001b[39min\u001b[39;00m LEXEME_NORM_LANGS:\n",
      "File \u001b[0;32m~/thesis24/lib/python3.11/site-packages/spacy/language.py:1052\u001b[0m, in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mselect_pipes\u001b[39m(\n\u001b[1;32m   1043\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1044\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[1;32m   1045\u001b[0m     disable: Optional[Union[\u001b[39mstr\u001b[39m, Iterable[\u001b[39mstr\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1046\u001b[0m     enable: Optional[Union[\u001b[39mstr\u001b[39m, Iterable[\u001b[39mstr\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1047\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mDisabledPipes\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1048\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Disable one or more pipeline components. If used as a context\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39m    manager, the pipeline will be restored to the initial state at the end\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m \u001b[39m    of the block. Otherwise, a DisabledPipes object is returned, that has\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m \u001b[39m    a `.restore()` method you can use to undo your changes.\u001b[39;00m\n\u001b[0;32m-> 1052\u001b[0m \n\u001b[1;32m   1053\u001b[0m \u001b[39m    disable (str or iterable): The name(s) of the pipes to disable\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[39m    enable (str or iterable): The name(s) of the pipes to enable - all others will be disabled\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \n\u001b[1;32m   1056\u001b[0m \u001b[39m    DOCS: https://spacy.io/api/language#select_pipes\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m     \u001b[39mif\u001b[39;00m enable \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m disable \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1059\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE991)\n",
      "\u001b[1;32m/Users/ahmed/Desktop/msc-24/TND/RQ2_2/bigger_study/facts_eval_3.ipynb Cell 23\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ahmed/Desktop/msc-24/TND/RQ2_2/bigger_study/facts_eval_3.ipynb#X61sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m@Language\u001b[39m\u001b[39m.\u001b[39mcomponent(\u001b[39m\"\u001b[39m\u001b[39mlegal_abbreviation_rules\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ahmed/Desktop/msc-24/TND/RQ2_2/bigger_study/facts_eval_3.ipynb#X61sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlegal_abbreviation_rules\u001b[39m(doc):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmed/Desktop/msc-24/TND/RQ2_2/bigger_study/facts_eval_3.ipynb#X61sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m# Define a set of common legal abbreviations\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmed/Desktop/msc-24/TND/RQ2_2/bigger_study/facts_eval_3.ipynb#X61sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     legal_abbreviations \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mArt.\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mSec.\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mMr.\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mMrs.\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDr.\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mMs.\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mProf.\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mGov.\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ahmed/Desktop/msc-24/TND/RQ2_2/bigger_study/facts_eval_3.ipynb#X61sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mfor\u001b[39;49;00m token \u001b[39min\u001b[39;49;00m doc[:\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]:  \u001b[39m# Iterate over the tokens but skip the last token\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmed/Desktop/msc-24/TND/RQ2_2/bigger_study/facts_eval_3.ipynb#X61sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         \u001b[39mif\u001b[39;49;00m token\u001b[39m.\u001b[39;49mtext \u001b[39min\u001b[39;49;00m legal_abbreviations \u001b[39mand\u001b[39;49;00m token\u001b[39m.\u001b[39;49mi \u001b[39m<\u001b[39;49m \u001b[39mlen\u001b[39;49m(doc) \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmed/Desktop/msc-24/TND/RQ2_2/bigger_study/facts_eval_3.ipynb#X61sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m             next_token \u001b[39m=\u001b[39;49m doc[token\u001b[39m.\u001b[39;49mi \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m]\n",
      "File \u001b[0;32m~/thesis24/lib/python3.11/site-packages/spacy/tokens/span.pyx:207\u001b[0m, in \u001b[0;36m__iter__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/thesis24/lib/python3.11/site-packages/spacy/tokens/doc.pyx:505\u001b[0m, in \u001b[0;36mspacy.tokens.doc.Doc.__getitem__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/thesis24/lib/python3.11/site-packages/spacy/tokens/token.pxd:25\u001b[0m, in \u001b[0;36mspacy.tokens.token.Token.cinit\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: [E040] Attempt to access token at 555, max length 555."
     ]
    }
   ],
   "source": [
    "sentences1 = extract_sentences(second_paragraph)\n",
    "sentences1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis24",
   "language": "python",
   "name": "thesis24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
