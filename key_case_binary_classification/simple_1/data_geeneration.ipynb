{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_sampling_and_copy(source_dir, target_dir, sample_size=4):\n",
    "    # Ensure target directory exists\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "    # Dictionary to hold files grouped by (year, importance)\n",
    "    grouped_files = defaultdict(list)\n",
    "\n",
    "    for file_name in os.listdir(source_dir):\n",
    "        if file_name.endswith(\".json\"):\n",
    "            file_path = os.path.join(source_dir, file_name)\n",
    "            \n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    data = json.load(file)\n",
    "\n",
    "                # Ensure required keys are present\n",
    "                if \"judgementdate\" in data and \"importance\" in data:\n",
    "                    # Convert judgementdate to a datetime object\n",
    "                    judgement_date = datetime.strptime(data[\"judgementdate\"], \"%d/%m/%Y\").date()\n",
    "                    year = judgement_date.year\n",
    "\n",
    "                    # Filter based on the given conditions\n",
    "                    if 2018 <= year <= 2024 and int(data[\"importance\"]) in [2, 3, 4]:\n",
    "                        # Group by (year, importance)\n",
    "                        grouped_files[(year, data[\"importance\"])].append(file_path)\n",
    "            except (json.JSONDecodeError, ValueError) as e:\n",
    "                print(f\"Error reading file {file_name}: {e}\")\n",
    "\n",
    "    # Collect all eligible files\n",
    "    all_filtered_files = [file for files in grouped_files.values() for file in files]\n",
    "\n",
    "    # Stratified sampling\n",
    "    sampled_files = []\n",
    "    files_per_group = max(1, sample_size // len(grouped_files))\n",
    "\n",
    "    for group, files in grouped_files.items():\n",
    "        sampled_files.extend(random.sample(files, min(files_per_group, len(files))))\n",
    "\n",
    "    # Add additional files to meet the target sample size\n",
    "    if len(sampled_files) < sample_size:\n",
    "        remaining_files = list(set(all_filtered_files) - set(sampled_files))\n",
    "        additional_samples = random.sample(remaining_files, min(sample_size - len(sampled_files), len(remaining_files)))\n",
    "        sampled_files.extend(additional_samples)\n",
    "\n",
    "    # Shuffle the final sampled files\n",
    "    random.shuffle(sampled_files)\n",
    "    # Copy sampled files to the target directory\n",
    "    for file_path in sampled_files:\n",
    "    #    shutil.copy(file_path, target_dir)\n",
    "        print(f\"Copied {os.path.basename(file_path)} to {target_dir}\")\n",
    "\n",
    "    print(f\"Successfully copied {len(sampled_files)} files to {target_dir}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied 001-201432.json to ../../kc_classification_data/125_notkc_2018_2024/\n",
      "Copied 001-199515.json to ../../kc_classification_data/125_notkc_2018_2024/\n",
      "Copied 001-212148.json to ../../kc_classification_data/125_notkc_2018_2024/\n",
      "Copied 001-184504.json to ../../kc_classification_data/125_notkc_2018_2024/\n",
      "Copied 001-217805.json to ../../kc_classification_data/125_notkc_2018_2024/\n",
      "Copied 001-213217.json to ../../kc_classification_data/125_notkc_2018_2024/\n",
      "Copied 001-189593.json to ../../kc_classification_data/125_notkc_2018_2024/\n",
      "Copied 001-223298.json to ../../kc_classification_data/125_notkc_2018_2024/\n",
      "Copied 001-194451.json to ../../kc_classification_data/125_notkc_2018_2024/\n",
      "Copied 001-180316.json to ../../kc_classification_data/125_notkc_2018_2024/\n",
      "Copied 001-217804.json to ../../kc_classification_data/125_notkc_2018_2024/\n",
      "Copied 001-222789.json to ../../kc_classification_data/125_notkc_2018_2024/\n",
      "Copied 001-203180.json to ../../kc_classification_data/125_notkc_2018_2024/\n",
      "Copied 001-204995.json to ../../kc_classification_data/125_notkc_2018_2024/\n",
      "Copied 001-210065.json to ../../kc_classification_data/125_notkc_2018_2024/\n",
      "Successfully copied 15 files to ../../kc_classification_data/125_notkc_2018_2024/.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Source and target directories\n",
    "source_directory = \"../../../ECHR/echr-processed/\"\n",
    "target_directory = \"../../kc_classification_data/125_notkc_2018_2024/\"\n",
    "\n",
    "# Execute the function\n",
    "stratified_sampling_and_copy(source_directory, target_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm ../../kc_classification_data/125_notkc_2018_2024/001-202345.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(source_dir, target_importance=2, size=125):\n",
    "    # Dictionary to hold files grouped by (year, importance)\n",
    "    grouped_files = {'date':[], 'file_path':[]}\n",
    "\n",
    "    for file_name in os.listdir(source_dir):\n",
    "        if file_name.endswith(\".json\"):\n",
    "            file_path = os.path.join(source_dir, file_name)\n",
    "            \n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    data = json.load(file)\n",
    "\n",
    "                # Ensure required keys are present\n",
    "                if \"judgementdate\" in data and \"importance\" in data and \"facts\" in data and \"law\" in data:\n",
    "                    # Convert judgementdate to a datetime object\n",
    "                    importance = int(data[\"importance\"])\n",
    "                    judgement_date = datetime.strptime(data[\"judgementdate\"], \"%d/%m/%Y\").date()\n",
    "                    file_id = file_path.split('/')[-1]\n",
    "\n",
    "                    # Filter based on the given conditions\n",
    "                    if importance == target_importance:\n",
    "                        # Group by (year, importance)\n",
    "                        grouped_files['date'].append(judgement_date)\n",
    "                        grouped_files['file_path'].append(file_id)\n",
    "\n",
    "            except (json.JSONDecodeError, ValueError) as e:\n",
    "                print(f\"Error reading file {file_name}: {e}\")\n",
    "    \n",
    "    grouped_files_df = pd.DataFrame.from_dict(grouped_files)\n",
    "    grouped_files_df = grouped_files_df.sort_values(by='date', ascending=False)\n",
    "    grouped_files_df = grouped_files_df.head(size)\n",
    "    return grouped_files_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_directory = \"../../../ECHR/echr-processed/\"\n",
    "df1 = collect_data(source_directory, target_importance=1)\n",
    "df2 = collect_data(source_directory, target_importance=2)\n",
    "df3 = collect_data(source_directory, target_importance=3)\n",
    "df4 = collect_data(source_directory, target_importance=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv('../../kc_classification_data/pre_cutoff_data/df_1.csv', index=False)\n",
    "df2.to_csv('../../kc_classification_data/pre_cutoff_data/df_2.csv', index=False)\n",
    "df3.to_csv('../../kc_classification_data/pre_cutoff_data/df_3.csv', index=False)\n",
    "#df4.to_csv('../../kc_classification_data/pre_cutoff_data/df_4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_directory = \"../../../ECHR/echr-processed/\"\n",
    "df4 = collect_data(source_directory, target_importance=4)\n",
    "df4.to_csv('../../kc_classification_data/pre_cutoff_data/df_4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
