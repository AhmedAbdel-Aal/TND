{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from typing import Dict\n",
    "\n",
    "class LegalCaseClassifier:\n",
    "    def __init__(self, model_name: str = \"gpt-4o\", temperature: float = 0):\n",
    "        self.llm = ChatOpenAI(model_name=model_name, temperature=temperature)\n",
    "        \n",
    "        # Step 1: Detailed Legal Analysis Prompt\n",
    "        self.legal_analysis_prompt = PromptTemplate(\n",
    "            input_variables=[\"case_facts\"],\n",
    "            template=\"\"\"You are an experienced legal expert specializing in European Court of Human Rights (ECHR) jurisprudence.\n",
    "            Analyze the following case facts through these specific steps:\n",
    "\n",
    "Case Facts:\n",
    "{case_facts}\n",
    "\n",
    "Conduct a systematic analysis:\n",
    "\n",
    "1. General Legal Norms and Principles:\n",
    "   - Identify the core legal principles and doctrines relevant to the case.\n",
    "   - Highlight any established legal precedents.\n",
    "\n",
    "2. Circumstances and Requirements:\n",
    "   - Match the specific facts of the case to the legal criteria or thresholds required for application.\n",
    "   - Note any conditions, exceptions, or thresholds explicitly mentioned.\n",
    "\n",
    "3. Scope and Application:\n",
    "   - Assess the case’s substantive or procedural impact.\n",
    "   - Identify whether the jurisdictional aspects (material, temporal) play a critical role.\n",
    "   - Pinpoint key qualifiers such as \"sometimes,\" \"exceptionally,\" or \"in the present case.\"\n",
    "\n",
    "4. Impact and Precedent:\n",
    "   - Determine whether the case introduces novel interpretations or applications of law.\n",
    "   - Evaluate how the case contributes to, clarifies, or modifies existing case law.\n",
    "\n",
    "5. Implications:\n",
    "   - Consider the case's broader implications beyond the immediate dispute.\n",
    "   - Highlight factors that indicate a significant contribution to legal development.\n",
    "\n",
    "Output the analysis in a step-by-step manner.\n",
    "\n",
    "Analysis:\n",
    "\"\"\"\n",
    "        )\n",
    "        \n",
    "        # Step 2: Classification Prompt with Clear Distinction\n",
    "        self.classification_prompt = PromptTemplate(\n",
    "            input_variables=[\"analysis\"],\n",
    "            template=\"\"\"You are an experienced legal expert specializing in European Court of Human Rights (ECHR) jurisprudence.\n",
    "            Based on the analysis, classify this case into one of these clearly defined categories:\n",
    "\n",
    "KEY CASE:\n",
    "- Makes a significant contribution to the development, clarification, or modification of case law.\n",
    "- Establishes new legal principles or substantially modifies existing ones.\n",
    "- Has broad implications beyond the immediate case.\n",
    "- Enlarges or narrows the application of an establieshed line of jurisprudence.\n",
    "\n",
    "NOT KEY CASE:\n",
    "- Applies existing case law without significant contributions to legal development.\n",
    "- Demonstrates limited implications beyond the immediate dispute.\n",
    "- Does not enlarge or narrow the application of an establieshed line of jurisprudence.\n",
    "\n",
    "Analysis provided:\n",
    "{analysis}\n",
    "\n",
    "Steps for Classification:\n",
    "1. Assess whether the case significantly develops, clarifies, or modifies case law.\n",
    "2. Determine if it establishes new legal principles or substantially modifies existing ones.\n",
    "3. Evaluate how the case might clarify, refine, or expand upon existing jurisprudence (e.g., enlarging or narrowing the application of established principles to new factual contexts).\n",
    "3. Evaluate the implications of the case (beyond the immediate context).\n",
    "\n",
    "Provide your reasoning step by step and conclude with one of the following exact classifications:\n",
    "\n",
    "CLASSIFICATION: KEY CASE\n",
    "CLASSIFICATION: NOT KEY CASE\n",
    "\"\"\"\n",
    "        )\n",
    "        \n",
    "        # Create the chains\n",
    "        self.legal_analysis_chain = LLMChain(llm=self.llm, prompt=self.legal_analysis_prompt)\n",
    "        self.classification_chain = LLMChain(llm=self.llm, prompt=self.classification_prompt)\n",
    "    \n",
    "    def classify_case(self, case_facts: str) -> Dict[str, any]:\n",
    "        \"\"\"\n",
    "        Classify a legal case based on its significance to case law development.\n",
    "        \n",
    "        Args:\n",
    "            case_facts: The facts section of the legal case\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing classification result and detailed analysis\n",
    "        \"\"\"\n",
    "        # Step 1: Generate legal analysis\n",
    "        legal_analysis = self.legal_analysis_chain.run(case_facts)\n",
    "        \n",
    "        # Step 2: Make classification based on analysis\n",
    "        classification_result = self.classification_chain.run(legal_analysis)\n",
    "        \n",
    "        # Extract final classification\n",
    "        if \"CLASSIFICATION: KEY CASE\" in classification_result:\n",
    "            classification = \"KEY CASE\"\n",
    "        elif \"CLASSIFICATION: NOT KEY CASE\" in classification_result:\n",
    "            classification = \"NOT KEY CASE\"\n",
    "        else:\n",
    "            classification = \"NOT KEY CASE\"\n",
    "        \n",
    "        return {\n",
    "            \"classification\": classification,\n",
    "            \"legal_analysis\": legal_analysis,\n",
    "            \"classification_reasoning\": classification_result\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from utils import load_json, save_json\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125, 125)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kc_input_dir = '../../kc_classification_data/125_kc_2018_2024/'\n",
    "not_kc_input_dir = '../../kc_classification_data/125_notkc_2018_2024/'\n",
    "\n",
    "kc_input_cases = os.listdir(kc_input_dir)\n",
    "not_kc_input_cases = os.listdir(not_kc_input_dir)\n",
    "\n",
    "output_dir = './results_3/'\n",
    "\n",
    "len(kc_input_cases), len(not_kc_input_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diabsle deprecation warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "already_processed_files = os.listdir(output_dir)\n",
    "\n",
    "# remove the prefix kc_ and not_kc_\n",
    "already_processed_files = [f.replace('kc_', '') for f in already_processed_files]\n",
    "already_processed_files = [f.replace('not', '') for f in already_processed_files]\n",
    "\n",
    "len(already_processed_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▎| 117/125 [1:01:43<04:13, 31.66s/it]\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Request too large for gpt-4o in organization org-2QVW8MeYCJElwjuh1GlaWvc3 on tokens per min (TPM): Limit 30000, Requested 38055. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/ahmed/Desktop/msc-24/TND/key_case_binary_classification/simple_1/CoT_binary_facts_try_3.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ahmed/Desktop/msc-24/TND/key_case_binary_classification/simple_1/CoT_binary_facts_try_3.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m#if len(case_facts) > 30600:\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ahmed/Desktop/msc-24/TND/key_case_binary_classification/simple_1/CoT_binary_facts_try_3.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m#    case_facts = case_facts[:30600]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmed/Desktop/msc-24/TND/key_case_binary_classification/simple_1/CoT_binary_facts_try_3.ipynb#W5sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m classifier \u001b[39m=\u001b[39m LegalCaseClassifier()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ahmed/Desktop/msc-24/TND/key_case_binary_classification/simple_1/CoT_binary_facts_try_3.ipynb#W5sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m result \u001b[39m=\u001b[39m classifier\u001b[39m.\u001b[39;49mclassify_case(case_facts)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmed/Desktop/msc-24/TND/key_case_binary_classification/simple_1/CoT_binary_facts_try_3.ipynb#W5sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m output_result \u001b[39m=\u001b[39m {}\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmed/Desktop/msc-24/TND/key_case_binary_classification/simple_1/CoT_binary_facts_try_3.ipynb#W5sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m output_result[\u001b[39m'\u001b[39m\u001b[39mresult\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m result\n",
      "\u001b[1;32m/Users/ahmed/Desktop/msc-24/TND/key_case_binary_classification/simple_1/CoT_binary_facts_try_3.ipynb Cell 6\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmed/Desktop/msc-24/TND/key_case_binary_classification/simple_1/CoT_binary_facts_try_3.ipynb#W5sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmed/Desktop/msc-24/TND/key_case_binary_classification/simple_1/CoT_binary_facts_try_3.ipynb#W5sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m \u001b[39mClassify a legal case based on its significance to case law development.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmed/Desktop/msc-24/TND/key_case_binary_classification/simple_1/CoT_binary_facts_try_3.ipynb#W5sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m \u001b[39m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmed/Desktop/msc-24/TND/key_case_binary_classification/simple_1/CoT_binary_facts_try_3.ipynb#W5sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m \u001b[39m    Dictionary containing classification result and detailed analysis\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmed/Desktop/msc-24/TND/key_case_binary_classification/simple_1/CoT_binary_facts_try_3.ipynb#W5sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmed/Desktop/msc-24/TND/key_case_binary_classification/simple_1/CoT_binary_facts_try_3.ipynb#W5sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m \u001b[39m# Step 1: Generate legal analysis\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ahmed/Desktop/msc-24/TND/key_case_binary_classification/simple_1/CoT_binary_facts_try_3.ipynb#W5sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m legal_analysis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlegal_analysis_chain\u001b[39m.\u001b[39;49mrun(case_facts)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmed/Desktop/msc-24/TND/key_case_binary_classification/simple_1/CoT_binary_facts_try_3.ipynb#W5sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m \u001b[39m# Step 2: Make classification based on analysis\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmed/Desktop/msc-24/TND/key_case_binary_classification/simple_1/CoT_binary_facts_try_3.ipynb#W5sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m classification_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassification_chain\u001b[39m.\u001b[39mrun(legal_analysis)\n",
      "File \u001b[0;32m~/hackatum24/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:182\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     warned \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     emit_warning()\n\u001b[0;32m--> 182\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/hackatum24/lib/python3.11/site-packages/langchain/chains/base.py:606\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    605\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`run` supports only one positional argument.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 606\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(args[\u001b[39m0\u001b[39;49m], callbacks\u001b[39m=\u001b[39;49mcallbacks, tags\u001b[39m=\u001b[39;49mtags, metadata\u001b[39m=\u001b[39;49mmetadata)[\n\u001b[1;32m    607\u001b[0m         _output_key\n\u001b[1;32m    608\u001b[0m     ]\n\u001b[1;32m    610\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m    611\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(kwargs, callbacks\u001b[39m=\u001b[39mcallbacks, tags\u001b[39m=\u001b[39mtags, metadata\u001b[39m=\u001b[39mmetadata)[\n\u001b[1;32m    612\u001b[0m         _output_key\n\u001b[1;32m    613\u001b[0m     ]\n",
      "File \u001b[0;32m~/hackatum24/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:182\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     warned \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     emit_warning()\n\u001b[0;32m--> 182\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/hackatum24/lib/python3.11/site-packages/langchain/chains/base.py:389\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \n\u001b[1;32m    359\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[39m        `Chain.output_keys`.\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    382\u001b[0m config \u001b[39m=\u001b[39m {\n\u001b[1;32m    383\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcallbacks\u001b[39m\u001b[39m\"\u001b[39m: callbacks,\n\u001b[1;32m    384\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtags\u001b[39m\u001b[39m\"\u001b[39m: tags,\n\u001b[1;32m    385\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m: metadata,\n\u001b[1;32m    386\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mrun_name\u001b[39m\u001b[39m\"\u001b[39m: run_name,\n\u001b[1;32m    387\u001b[0m }\n\u001b[0;32m--> 389\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minvoke(\n\u001b[1;32m    390\u001b[0m     inputs,\n\u001b[1;32m    391\u001b[0m     cast(RunnableConfig, {k: v \u001b[39mfor\u001b[39;49;00m k, v \u001b[39min\u001b[39;49;00m config\u001b[39m.\u001b[39;49mitems() \u001b[39mif\u001b[39;49;00m v \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m}),\n\u001b[1;32m    392\u001b[0m     return_only_outputs\u001b[39m=\u001b[39;49mreturn_only_outputs,\n\u001b[1;32m    393\u001b[0m     include_run_info\u001b[39m=\u001b[39;49minclude_run_info,\n\u001b[1;32m    394\u001b[0m )\n",
      "File \u001b[0;32m~/hackatum24/lib/python3.11/site-packages/langchain/chains/base.py:170\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    169\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 170\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    171\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    173\u001b[0m \u001b[39mif\u001b[39;00m include_run_info:\n",
      "File \u001b[0;32m~/hackatum24/lib/python3.11/site-packages/langchain/chains/base.py:160\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    159\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 160\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    161\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    162\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    165\u001b[0m     final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    166\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[1;32m    167\u001b[0m     )\n\u001b[1;32m    168\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/hackatum24/lib/python3.11/site-packages/langchain/chains/llm.py:126\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\n\u001b[1;32m    122\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    123\u001b[0m     inputs: Dict[\u001b[39mstr\u001b[39m, Any],\n\u001b[1;32m    124\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    125\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m--> 126\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate([inputs], run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    127\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_outputs(response)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/hackatum24/lib/python3.11/site-packages/langchain/chains/llm.py:138\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    136\u001b[0m callbacks \u001b[39m=\u001b[39m run_manager\u001b[39m.\u001b[39mget_child() \u001b[39mif\u001b[39;00m run_manager \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm, BaseLanguageModel):\n\u001b[0;32m--> 138\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm\u001b[39m.\u001b[39;49mgenerate_prompt(\n\u001b[1;32m    139\u001b[0m         prompts,\n\u001b[1;32m    140\u001b[0m         stop,\n\u001b[1;32m    141\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    142\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm_kwargs,\n\u001b[1;32m    143\u001b[0m     )\n\u001b[1;32m    144\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm\u001b[39m.\u001b[39mbind(stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm_kwargs)\u001b[39m.\u001b[39mbatch(\n\u001b[1;32m    146\u001b[0m         cast(List, prompts), {\u001b[39m\"\u001b[39m\u001b[39mcallbacks\u001b[39m\u001b[39m\"\u001b[39m: callbacks}\n\u001b[1;32m    147\u001b[0m     )\n",
      "File \u001b[0;32m~/hackatum24/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:786\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_prompt\u001b[39m(\n\u001b[1;32m    779\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    780\u001b[0m     prompts: \u001b[39mlist\u001b[39m[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    784\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    785\u001b[0m     prompt_messages \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mto_messages() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[0;32m--> 786\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(prompt_messages, stop\u001b[39m=\u001b[39;49mstop, callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/hackatum24/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:643\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n\u001b[1;32m    642\u001b[0m             run_managers[i]\u001b[39m.\u001b[39mon_llm_error(e, response\u001b[39m=\u001b[39mLLMResult(generations\u001b[39m=\u001b[39m[]))\n\u001b[0;32m--> 643\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    644\u001b[0m flattened_outputs \u001b[39m=\u001b[39m [\n\u001b[1;32m    645\u001b[0m     LLMResult(generations\u001b[39m=\u001b[39m[res\u001b[39m.\u001b[39mgenerations], llm_output\u001b[39m=\u001b[39mres\u001b[39m.\u001b[39mllm_output)  \u001b[39m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    646\u001b[0m     \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results\n\u001b[1;32m    647\u001b[0m ]\n\u001b[1;32m    648\u001b[0m llm_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_combine_llm_outputs([res\u001b[39m.\u001b[39mllm_output \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/hackatum24/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:633\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mfor\u001b[39;00m i, m \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(messages):\n\u001b[1;32m    631\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m         results\u001b[39m.\u001b[39mappend(\n\u001b[0;32m--> 633\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_with_cache(\n\u001b[1;32m    634\u001b[0m                 m,\n\u001b[1;32m    635\u001b[0m                 stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    636\u001b[0m                 run_manager\u001b[39m=\u001b[39;49mrun_managers[i] \u001b[39mif\u001b[39;49;00m run_managers \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    637\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    638\u001b[0m             )\n\u001b[1;32m    639\u001b[0m         )\n\u001b[1;32m    640\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    641\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/hackatum24/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:851\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    850\u001b[0m     \u001b[39mif\u001b[39;00m inspect\u001b[39m.\u001b[39msignature(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 851\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[1;32m    852\u001b[0m             messages, stop\u001b[39m=\u001b[39;49mstop, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    853\u001b[0m         )\n\u001b[1;32m    854\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    855\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(messages, stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/hackatum24/lib/python3.11/site-packages/langchain_community/chat_models/openai.py:476\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    470\u001b[0m message_dicts, params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[1;32m    471\u001b[0m params \u001b[39m=\u001b[39m {\n\u001b[1;32m    472\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    473\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m({\u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m: stream} \u001b[39mif\u001b[39;00m stream \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {}),\n\u001b[1;32m    474\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    475\u001b[0m }\n\u001b[0;32m--> 476\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompletion_with_retry(\n\u001b[1;32m    477\u001b[0m     messages\u001b[39m=\u001b[39;49mmessage_dicts, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams\n\u001b[1;32m    478\u001b[0m )\n\u001b[1;32m    479\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[0;32m~/hackatum24/lib/python3.11/site-packages/langchain_community/chat_models/openai.py:387\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry\u001b[0;34m(self, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Use tenacity to retry the completion call.\"\"\"\u001b[39;00m\n\u001b[1;32m    386\u001b[0m \u001b[39mif\u001b[39;00m is_openai_v1():\n\u001b[0;32m--> 387\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    389\u001b[0m retry_decorator \u001b[39m=\u001b[39m _create_retry_decorator(\u001b[39mself\u001b[39m, run_manager\u001b[39m=\u001b[39mrun_manager)\n\u001b[1;32m    391\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m    392\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n",
      "File \u001b[0;32m~/hackatum24/lib/python3.11/site-packages/openai/_utils/_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m             msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing required argument: \u001b[39m\u001b[39m{\u001b[39;00mquote(missing[\u001b[39m0\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 275\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/hackatum24/lib/python3.11/site-packages/openai/resources/chat/completions.py:829\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[39m@required_args\u001b[39m([\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], [\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    789\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    790\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    826\u001b[0m     timeout: \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m httpx\u001b[39m.\u001b[39mTimeout \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m|\u001b[39m NotGiven \u001b[39m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    827\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatCompletion \u001b[39m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    828\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 829\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(\n\u001b[1;32m    830\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39m/chat/completions\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    831\u001b[0m         body\u001b[39m=\u001b[39;49mmaybe_transform(\n\u001b[1;32m    832\u001b[0m             {\n\u001b[1;32m    833\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmessages\u001b[39;49m\u001b[39m\"\u001b[39;49m: messages,\n\u001b[1;32m    834\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m: model,\n\u001b[1;32m    835\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39maudio\u001b[39;49m\u001b[39m\"\u001b[39;49m: audio,\n\u001b[1;32m    836\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfrequency_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: frequency_penalty,\n\u001b[1;32m    837\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunction_call\u001b[39;49m\u001b[39m\"\u001b[39;49m: function_call,\n\u001b[1;32m    838\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunctions\u001b[39;49m\u001b[39m\"\u001b[39;49m: functions,\n\u001b[1;32m    839\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogit_bias\u001b[39;49m\u001b[39m\"\u001b[39;49m: logit_bias,\n\u001b[1;32m    840\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogprobs\u001b[39;49m\u001b[39m\"\u001b[39;49m: logprobs,\n\u001b[1;32m    841\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmax_completion_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_completion_tokens,\n\u001b[1;32m    842\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmax_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_tokens,\n\u001b[1;32m    843\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmetadata\u001b[39;49m\u001b[39m\"\u001b[39;49m: metadata,\n\u001b[1;32m    844\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmodalities\u001b[39;49m\u001b[39m\"\u001b[39;49m: modalities,\n\u001b[1;32m    845\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mn\u001b[39;49m\u001b[39m\"\u001b[39;49m: n,\n\u001b[1;32m    846\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mparallel_tool_calls\u001b[39;49m\u001b[39m\"\u001b[39;49m: parallel_tool_calls,\n\u001b[1;32m    847\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mprediction\u001b[39;49m\u001b[39m\"\u001b[39;49m: prediction,\n\u001b[1;32m    848\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mpresence_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: presence_penalty,\n\u001b[1;32m    849\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mresponse_format\u001b[39;49m\u001b[39m\"\u001b[39;49m: response_format,\n\u001b[1;32m    850\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m: seed,\n\u001b[1;32m    851\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mservice_tier\u001b[39;49m\u001b[39m\"\u001b[39;49m: service_tier,\n\u001b[1;32m    852\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstop\u001b[39;49m\u001b[39m\"\u001b[39;49m: stop,\n\u001b[1;32m    853\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstore\u001b[39;49m\u001b[39m\"\u001b[39;49m: store,\n\u001b[1;32m    854\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstream\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream,\n\u001b[1;32m    855\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstream_options\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream_options,\n\u001b[1;32m    856\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m: temperature,\n\u001b[1;32m    857\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtool_choice\u001b[39;49m\u001b[39m\"\u001b[39;49m: tool_choice,\n\u001b[1;32m    858\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtools\u001b[39;49m\u001b[39m\"\u001b[39;49m: tools,\n\u001b[1;32m    859\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_logprobs\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_logprobs,\n\u001b[1;32m    860\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_p\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_p,\n\u001b[1;32m    861\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m: user,\n\u001b[1;32m    862\u001b[0m             },\n\u001b[1;32m    863\u001b[0m             completion_create_params\u001b[39m.\u001b[39;49mCompletionCreateParams,\n\u001b[1;32m    864\u001b[0m         ),\n\u001b[1;32m    865\u001b[0m         options\u001b[39m=\u001b[39;49mmake_request_options(\n\u001b[1;32m    866\u001b[0m             extra_headers\u001b[39m=\u001b[39;49mextra_headers, extra_query\u001b[39m=\u001b[39;49mextra_query, extra_body\u001b[39m=\u001b[39;49mextra_body, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    867\u001b[0m         ),\n\u001b[1;32m    868\u001b[0m         cast_to\u001b[39m=\u001b[39;49mChatCompletion,\n\u001b[1;32m    869\u001b[0m         stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    870\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mStream[ChatCompletionChunk],\n\u001b[1;32m    871\u001b[0m     )\n",
      "File \u001b[0;32m~/hackatum24/lib/python3.11/site-packages/openai/_base_client.py:1278\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\n\u001b[1;32m   1265\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1266\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1273\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1274\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[1;32m   1275\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[1;32m   1276\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[1;32m   1277\u001b[0m     )\n\u001b[0;32m-> 1278\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(cast_to, opts, stream\u001b[39m=\u001b[39;49mstream, stream_cls\u001b[39m=\u001b[39;49mstream_cls))\n",
      "File \u001b[0;32m~/hackatum24/lib/python3.11/site-packages/openai/_base_client.py:955\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    953\u001b[0m     retries_taken \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 955\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    956\u001b[0m     cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m    957\u001b[0m     options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    958\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    959\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    960\u001b[0m     retries_taken\u001b[39m=\u001b[39;49mretries_taken,\n\u001b[1;32m    961\u001b[0m )\n",
      "File \u001b[0;32m~/hackatum24/lib/python3.11/site-packages/openai/_base_client.py:1044\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[39mif\u001b[39;00m remaining_retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_retry(err\u001b[39m.\u001b[39mresponse):\n\u001b[1;32m   1043\u001b[0m     err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mclose()\n\u001b[0;32m-> 1044\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retry_request(\n\u001b[1;32m   1045\u001b[0m         input_options,\n\u001b[1;32m   1046\u001b[0m         cast_to,\n\u001b[1;32m   1047\u001b[0m         retries_taken\u001b[39m=\u001b[39;49mretries_taken,\n\u001b[1;32m   1048\u001b[0m         response_headers\u001b[39m=\u001b[39;49merr\u001b[39m.\u001b[39;49mresponse\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m   1049\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m   1050\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m   1051\u001b[0m     )\n\u001b[1;32m   1053\u001b[0m \u001b[39m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[39m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/hackatum24/lib/python3.11/site-packages/openai/_base_client.py:1093\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1089\u001b[0m \u001b[39m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1090\u001b[0m \u001b[39m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m time\u001b[39m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1093\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m   1094\u001b[0m     options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m   1095\u001b[0m     cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m   1096\u001b[0m     retries_taken\u001b[39m=\u001b[39;49mretries_taken \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m,\n\u001b[1;32m   1097\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m   1098\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m   1099\u001b[0m )\n",
      "File \u001b[0;32m~/hackatum24/lib/python3.11/site-packages/openai/_base_client.py:1044\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[39mif\u001b[39;00m remaining_retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_retry(err\u001b[39m.\u001b[39mresponse):\n\u001b[1;32m   1043\u001b[0m     err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mclose()\n\u001b[0;32m-> 1044\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retry_request(\n\u001b[1;32m   1045\u001b[0m         input_options,\n\u001b[1;32m   1046\u001b[0m         cast_to,\n\u001b[1;32m   1047\u001b[0m         retries_taken\u001b[39m=\u001b[39;49mretries_taken,\n\u001b[1;32m   1048\u001b[0m         response_headers\u001b[39m=\u001b[39;49merr\u001b[39m.\u001b[39;49mresponse\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m   1049\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m   1050\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m   1051\u001b[0m     )\n\u001b[1;32m   1053\u001b[0m \u001b[39m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[39m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/hackatum24/lib/python3.11/site-packages/openai/_base_client.py:1093\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1089\u001b[0m \u001b[39m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1090\u001b[0m \u001b[39m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m time\u001b[39m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1093\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m   1094\u001b[0m     options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m   1095\u001b[0m     cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m   1096\u001b[0m     retries_taken\u001b[39m=\u001b[39;49mretries_taken \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m,\n\u001b[1;32m   1097\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m   1098\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m   1099\u001b[0m )\n",
      "File \u001b[0;32m~/hackatum24/lib/python3.11/site-packages/openai/_base_client.py:1059\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1056\u001b[0m         err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mread()\n\u001b[1;32m   1058\u001b[0m     log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mRe-raising status error\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1059\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_status_error_from_response(err\u001b[39m.\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1061\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_response(\n\u001b[1;32m   1062\u001b[0m     cast_to\u001b[39m=\u001b[39mcast_to,\n\u001b[1;32m   1063\u001b[0m     options\u001b[39m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1067\u001b[0m     retries_taken\u001b[39m=\u001b[39mretries_taken,\n\u001b[1;32m   1068\u001b[0m )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Request too large for gpt-4o in organization org-2QVW8MeYCJElwjuh1GlaWvc3 on tokens per min (TPM): Limit 30000, Requested 38055. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "for case in tqdm(kc_input_cases):\n",
    "    if case in already_processed_files:\n",
    "        continue\n",
    "    case_path = os.path.join(kc_input_dir, case)\n",
    "    case_data = load_json(case_path)\n",
    "    case_facts = case_data['case_text']['THE FACTS']\n",
    "\n",
    "    #if len(case_facts) > 30600:\n",
    "    #    case_facts = case_facts[:30600]\n",
    "\n",
    "    classifier = LegalCaseClassifier()\n",
    "    result = classifier.classify_case(case_facts)\n",
    "\n",
    "    output_result = {}\n",
    "    output_result['result'] = result\n",
    "    output_result['id'] = case\n",
    "    output_result['ground_truth'] = 'KEY CASE'\n",
    "    output_result['one-liner'] = case_data['one-liner']\n",
    "    save_json(f'./results_3/kc_{case}', output_result)\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for case in tqdm(not_kc_input_cases):\n",
    "    #if case in already_processed_files:\n",
    "    #    continue\n",
    "    case_path = os.path.join(not_kc_input_dir, case)\n",
    "    case_data = load_json(case_path)\n",
    "    if 'facts' not in case_data:\n",
    "        print('facts not found in the case with name:', case)\n",
    "        continue\n",
    "    case_facts = case_data['facts']\n",
    "\n",
    "    #if len(case_facts) > 30600:\n",
    "    #    case_facts = case_facts[:30600]\n",
    "\n",
    "    classifier = LegalCaseClassifier()\n",
    "    result = classifier.classify_case(case_facts)\n",
    "\n",
    "    output_result = {}\n",
    "    output_result['result'] = result\n",
    "    output_result['id'] = case\n",
    "    output_result['ground_truth'] = ' NOT KEY CASE'\n",
    "    output_result['importance'] = case_data['importance']\n",
    "    save_json(f'./results_3/notkc_{case}', output_result)\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key case = 1\n",
    "# not key case = 0\n",
    "\n",
    "# load all files in result and create a dataframe with the following columns \n",
    "# id, result.classification, ground_truth\n",
    "# save the dataframe as csv\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def return_binary_classification(result):\n",
    "    if result == 'KEY CASE':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "results_dir = './results_3/'\n",
    "results = os.listdir(results_dir)\n",
    "\n",
    "data = []\n",
    "for result in results:\n",
    "    result_data = load_json(os.path.join(results_dir, result))\n",
    "    data.append({\n",
    "        'id': result_data['id'],\n",
    "        'classification': return_binary_classification(result_data['result']['classification']),\n",
    "        'ground_truth': return_binary_classification(result_data['ground_truth']),\n",
    "        'cutoff_date': 0.5\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4358974358974359, 1.0, 0.4358974358974359, 0.6071428571428571)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "accuracy = (df['classification'] == df['ground_truth']).sum() / len(df)\n",
    "\n",
    "# calculate precision\n",
    "precision = (df['classification'] & df['ground_truth']).sum() / df['classification'].sum()\n",
    "\n",
    "# calculate recall\n",
    "recall = (df['classification'] & df['ground_truth']).sum() / df['ground_truth'].sum()\n",
    "\n",
    "# calculate f1 score\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='ground_truth', ylabel='classification'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# heatmap for columnd: classification, ground_truth\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "confusion_matrix = pd.crosstab(df['classification'], df['ground_truth'], rownames=['classification'], colnames=['ground_truth'])\n",
    "sns.heatmap(confusion_matrix, annot=True, cmap='Blues', fmt='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackatum24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
